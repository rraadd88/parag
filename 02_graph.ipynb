{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Pairwise) Graph\n",
    "\n",
    "> For graph layoout related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import pandas as pd\n",
    "from roux.lib.io import read_dict, to_dict, read_ps, read_table, to_table\n",
    "import roux.lib.df as rd  # noqa\n",
    "\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "from roux.lib.str import linebreaker\n",
    "\n",
    "\n",
    "def center_subset_label(\n",
    "    ds1: pd.Series,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Removes the subset label except at the middle.\n",
    "    \"\"\"\n",
    "    i = ds1.index.tolist()[len(ds1) // 2]\n",
    "    # print(i,ds1.index)\n",
    "    k = ds1.loc[i]\n",
    "    ds1.loc[:] = \"\"\n",
    "    ds1.loc[i] = linebreaker(k, 10)\n",
    "    return ds1\n",
    "\n",
    "\n",
    "def get_preprocessed(\n",
    "    nodes: pd.DataFrame,\n",
    "    edges: pd.DataFrame,\n",
    "    col_node_id: str,\n",
    "    col_node_name: str,\n",
    "    col_subset_id: str,\n",
    "    col_subset_name: str,\n",
    "    _col_node_id: str,\n",
    "    _col_node_name: str,\n",
    "    _col_node_parent: str,\n",
    "    _col_source: str,\n",
    "    _col_target: str,\n",
    "    _col_subset_name: str,\n",
    "    col_source: str,\n",
    "    col_target: str,\n",
    "    remove_orphans: bool,\n",
    "    testn: int = None,\n",
    "    verbose: bool = False,\n",
    ") -> tuple:\n",
    "    \"\"\"Get preprocessed nodes and edges\n",
    "\n",
    "    Args:\n",
    "        nodes (pd.DataFrame): node data\n",
    "        edges (pd.DataFrame): edge data\n",
    "        col_node_id (str): node id\n",
    "        col_node_name (str): node name\n",
    "        col_source (str): source column\n",
    "        col_target (str): target column\n",
    "        col_subset_id (str): subset id\n",
    "        col_subset_name (str): subset name\n",
    "        _col_node_id (str): vega node id\n",
    "        _col_node_name (str): vega node name\n",
    "        _col_node_parent (str): vega parent key\n",
    "        _col_source (str): vega source key\n",
    "        _col_target (str): vega source key\n",
    "        _col_subset_name (str): vega subset name key\n",
    "        remove_orphans (bool): remove orphan/unconnected nodes\n",
    "        testn (int, optional): test n nodes. Defaults to None.\n",
    "        verbose (bool, optional): verbose. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: nodes, edges\n",
    "    \"\"\"\n",
    "    df1 = nodes.loc[\n",
    "        :, list(set([col_node_id, col_node_name, col_subset_id, col_subset_name]))\n",
    "    ].log.drop_duplicates(\n",
    "        subset=list(\n",
    "            set(\n",
    "                [\n",
    "                    col_node_id,\n",
    "                    col_node_name,\n",
    "                    # col_node_parent,\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    df2 = (\n",
    "        edges.loc[:, [col_source, col_target]]\n",
    "        .drop_duplicates()\n",
    "        .rename(\n",
    "            columns={\n",
    "                col_source: _col_source,\n",
    "                col_target: _col_target,\n",
    "            },\n",
    "        )\n",
    "        .astype(\n",
    "            {\n",
    "                # _col_source: int,\n",
    "                # _col_target: int,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    if not testn is None:\n",
    "        df1 = df1.head(testn)\n",
    "        logging.warning(f\"testing {testn} nodes\")\n",
    "    if verbose:\n",
    "        print(df1.head())\n",
    "\n",
    "    ## reset the node indexing\n",
    "    ## need to start from 2\n",
    "    nodes = df1[col_node_id].unique().tolist()\n",
    "    if verbose:\n",
    "        print(nodes)\n",
    "    if remove_orphans:\n",
    "        if verbose:\n",
    "            print(len(nodes))\n",
    "        nodes = [\n",
    "            i\n",
    "            for i in set(df2[_col_source].tolist() + df2[_col_target].tolist())\n",
    "            if i in nodes\n",
    "        ]\n",
    "        if verbose:\n",
    "            print(f\"filtered by edges: {len(nodes)}\")\n",
    "        # df1=df1.log.query(expr=f\"`{col_node_id}` in {nodes}\")\n",
    "\n",
    "    to_node_index = dict(zip(nodes, range(4, len(nodes) + 4)))\n",
    "\n",
    "    df1 = (\n",
    "        df1.assign(**{_col_node_id: lambda df: df[col_node_id].map(to_node_index)})\n",
    "        .rename(\n",
    "            columns={\n",
    "                col_node_name: _col_node_name,\n",
    "            },\n",
    "        )\n",
    "        .astype(\n",
    "            {\n",
    "                # _col_node_id: int,\n",
    "                # _col_node_parent: int,\n",
    "            }\n",
    "        )\n",
    "        # .astype(\n",
    "        #     {\n",
    "        #         _col_node_name: str,\n",
    "        #     }\n",
    "        # )\n",
    "        # .sort_values(_col_node_name)\n",
    "        # .rename(\n",
    "        #     columns={\n",
    "        #         col_node_id:_col_node_id,\n",
    "        #         # col_node_parent:_col_node_parent,\n",
    "        #     },\n",
    "        #     errors='raise',\n",
    "        # )\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\n",
    "            df1.loc[\n",
    "                :,\n",
    "                list(\n",
    "                    set([_col_node_id, _col_node_name, col_subset_id, col_subset_name])\n",
    "                ),\n",
    "            ]\n",
    "        ).head()\n",
    "        print(df1.columns.tolist())\n",
    "\n",
    "    ## subsets\n",
    "    df1 = df1.astype({col_subset_id: str})\n",
    "    df1 = (\n",
    "        df1.log.dropna(\n",
    "            subset=list(\n",
    "                set([_col_node_id, _col_node_name, col_subset_id, col_subset_name])\n",
    "            )\n",
    "        )\n",
    "        # .sort_values([col_subset_name,_col_node_name])\n",
    "        .groupby(\n",
    "            col_subset_id,\n",
    "            group_keys=False,\n",
    "            sort=False,\n",
    "        )\n",
    "        .apply(\n",
    "            lambda df: df.assign(\n",
    "                **{\n",
    "                    _col_subset_name: lambda df_: center_subset_label(\n",
    "                        df_[col_subset_name].copy()\n",
    "                    ),\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ## add the placeholder without parent\n",
    "    df1_ = pd.DataFrame(\n",
    "        {\n",
    "            _col_node_id: [\n",
    "                1,\n",
    "                2,\n",
    "                3,\n",
    "            ],\n",
    "            _col_node_name: [\n",
    "                \"1\",\n",
    "                \"2\",\n",
    "                \"3\",\n",
    "            ],\n",
    "            _col_node_parent: [\n",
    "                None,\n",
    "                1,\n",
    "                2,\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    df1 = df1.assign(**{_col_node_parent: 3}).astype(\n",
    "        {\n",
    "            # _col_node_id: int,\n",
    "            # _col_node_name: str,\n",
    "            # _col_node_parent: int,\n",
    "        }\n",
    "    )\n",
    "    df1 = pd.concat(\n",
    "        [df1_, df1],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(df1.iloc[5, :].T.to_dict())\n",
    "    df1 = df1.fillna({_col_node_name: df1[_col_node_id].astype(int).astype(str)})\n",
    "\n",
    "    if verbose:\n",
    "        print(df1.head(1 if testn is None else testn))\n",
    "        print(df1[_col_node_id].nunique())\n",
    "\n",
    "    ## edges\n",
    "    df2 = (\n",
    "        df2\n",
    "        # filter by nodes\n",
    "        .log.query(expr=f\"{_col_source} in {nodes} and {_col_target} in {nodes}\")\n",
    "        .assign(\n",
    "            **{\n",
    "                _col_source: lambda df: df[_col_source].map(to_node_index),\n",
    "                _col_target: lambda df: df[_col_target].map(to_node_index),\n",
    "            }\n",
    "        )\n",
    "        .rd.assert_no_na()\n",
    "    )\n",
    "    return df1, df2, to_node_index\n",
    "\n",
    "\n",
    "def to_net(\n",
    "    col_node_id: str,\n",
    "    col_source: str,\n",
    "    col_target: str,\n",
    "    col_subset_id: str,\n",
    "    # data\n",
    "    nodes: pd.DataFrame = None,\n",
    "    edges: pd.DataFrame = None,\n",
    "    nodes_path: str = None,\n",
    "    edges_path: str = None,\n",
    "    # labels\n",
    "    col_node_name: str = None,\n",
    "    # col_node_parent='parent',\n",
    "    col_subset_name: str = None,  #'gene set name',\n",
    "    ## aes\n",
    "    cmap_subsets=None,  # 'nipy_spectral'\n",
    "    ## switches\n",
    "    remove_orphans: bool = True,\n",
    "    show_node_names: bool = True,\n",
    "    ## knobs\n",
    "    off_subset_name: int = 15,\n",
    "    ## io\n",
    "    config_base_path: str = None,  # 'inputs/edge_bundle.json',\n",
    "    defaults: dict = None,\n",
    "    use_urls: bool = False,\n",
    "    out: str = None,\n",
    "    plot: bool = True,\n",
    "    ## etc\n",
    "    testn: int = None,\n",
    "    verbose: bool = False,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Plots the interactive pairwise graph.\n",
    "\n",
    "    Args:\n",
    "        nodes (pd.DataFrame): node data\n",
    "        edges (pd.DataFrame): edge data\n",
    "        col_node_id (str): node id\n",
    "        col_node_name (str): node name\n",
    "        col_source (str): source column\n",
    "        col_target (str): target column\n",
    "        col_subset_id (str): subset id\n",
    "        col_subset_name (str): subset name\n",
    "        nodes_path (str, optional): path to nodes file. Defaults to None.\n",
    "        edges_path (str, optional): path to edges file. Defaults to None.\n",
    "        cmap_subsets (_type_, optional): colormap. Defaults to None.\n",
    "        show_node_names (bool, optional): show node names. Defaults to True.\n",
    "        off_subset_name (int, optional): offset of the subset names from the node names. Defaults to 15.\n",
    "        config_base_path (str, optional): vega config path. Defaults to None.\n",
    "        defaults (dict, optional): default vega settings. Defaults to None.\n",
    "        use_urls (bool, optional): use urls in the vega config. Defaults to False.\n",
    "        out (str, optional): output format. Defaults to None.\n",
    "        plot (bool, optional): plot or not. Defaults to True.\n",
    "        testn (int, optional): test n nodes. Defaults to None.\n",
    "        verbose (bool, optional): verbose. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: vega config, nodes\n",
    "    \"\"\"\n",
    "    _col_node_id = \"id\"\n",
    "    _col_node_name = \"name\"\n",
    "    _col_node_parent = \"parent\"\n",
    "    _col_source = \"source\"\n",
    "    _col_target = \"target\"\n",
    "    _col_subset_name = \"subset name\"\n",
    "\n",
    "    if config_base_path is None:\n",
    "        from parag.utils import get_src_path\n",
    "\n",
    "        config_base_path = f\"{get_src_path()}/data/edge_bundle_subsets.json\"\n",
    "    d1 = read_dict(config_base_path)\n",
    "    d1[\"marks\"][1][\"encode\"][\"update\"][\"dx\"][\n",
    "        \"signal\"\n",
    "    ] = f\"textOffset * (datum.leftside ? -{off_subset_name} : {off_subset_name})\"\n",
    "\n",
    "    ## nodes\n",
    "    if col_node_name is None:\n",
    "        col_node_name = col_node_id\n",
    "        # _col_node_name=_col_node_id\n",
    "    if col_subset_name is None:\n",
    "        col_subset_name = col_subset_id\n",
    "    if not nodes_path is None and not edges_path is None:\n",
    "        df1 = pd.DataFrame(read_dict(nodes_path))\n",
    "        df2 = pd.DataFrame(read_dict(edges_path))\n",
    "    else:\n",
    "        df1, df2, to_node_index = get_preprocessed(\n",
    "            nodes,\n",
    "            edges,\n",
    "            col_node_id=col_node_id,\n",
    "            col_node_name=col_node_name,\n",
    "            col_subset_id=col_subset_id,\n",
    "            col_subset_name=col_subset_name,\n",
    "            _col_node_parent=_col_node_parent,\n",
    "            _col_node_id=_col_node_id,\n",
    "            _col_node_name=_col_node_name,\n",
    "            col_source=col_source,\n",
    "            col_target=col_target,\n",
    "            _col_source=_col_source,\n",
    "            _col_target=_col_target,\n",
    "            _col_subset_name=_col_subset_name,\n",
    "            testn=testn,\n",
    "            verbose=verbose,\n",
    "            remove_orphans=remove_orphans,\n",
    "        )\n",
    "\n",
    "    # ## reset the parent indexing\n",
    "    # ## need to start from 1\n",
    "    # parents=df1[_col_node_parent].unique().tolist()\n",
    "    # to_parent_index=dict(zip(parents,range(4,len(parents)+4)))\n",
    "    # df1[_col_node_parent]=df1[_col_node_parent].map(to_parent_index)\n",
    "    # return df1\n",
    "    ## color by subset\n",
    "    from parag.utils import get_colors\n",
    "\n",
    "    to_subset_color = get_colors(\n",
    "        subsets=df1[col_subset_id].dropna().unique(),  # todo use hue_order\n",
    "    )\n",
    "    if verbose:\n",
    "        print(to_subset_color)\n",
    "    d1[\"marks\"][0][\"encode\"][\"update\"][\"fill\"] = [\n",
    "        {\"test\": f\"datum.id === {str(int(k))}\", \"value\": v}\n",
    "        for k, v in df1.set_index(_col_node_id)[col_subset_id]\n",
    "        .map(to_subset_color)\n",
    "        .to_dict()\n",
    "        .items()\n",
    "    ] + d1[\"marks\"][0][\"encode\"][\"update\"][\"fill\"]\n",
    "    d1[\"marks\"][1][\"encode\"][\"update\"][\"fill\"] = [\n",
    "        {\"test\": f\"datum.id === {str(int(k))}\", \"value\": v}\n",
    "        for k, v in df1.set_index(_col_node_id)[col_subset_id]\n",
    "        .map(to_subset_color)\n",
    "        .to_dict()\n",
    "        .items()\n",
    "    ] + d1[\"marks\"][1][\"encode\"][\"update\"][\"fill\"]\n",
    "\n",
    "    ### save\n",
    "    # print(df1[_col_node_parent].value_counts().loc[[1,2]]==1)\n",
    "    # d1['marks'][0]['encode']['update']['fill']\n",
    "    ## remove other columns\n",
    "    # df1=df1.loc[:,[_col_node_id,_col_node_name,\n",
    "    #                 # col_node_parent,\n",
    "    #                ]]\n",
    "    assert all(df1[_col_node_parent].value_counts().loc[[1, 2]] == 1)\n",
    "    assert df1[_col_node_parent].isnull().sum() == 1\n",
    "    if verbose:\n",
    "        print(df2.head(1 if testn is None else testn))\n",
    "        print(len(df2))\n",
    "\n",
    "    if use_urls:\n",
    "        d1[\"data\"][0][\"url\"] = to_dict(\n",
    "            df1.T.apply(lambda x: x.dropna().to_dict()).tolist(),\n",
    "            \"vega/nodes.json\",\n",
    "        )\n",
    "\n",
    "        d1[\"data\"][-2][\"url\"] = to_dict(\n",
    "            df2.T.apply(lambda x: x.dropna().to_dict()).tolist(),\n",
    "            \"vega/edges.json\",\n",
    "        )\n",
    "    else:\n",
    "        del d1[\"data\"][0][\"url\"]\n",
    "        d1[\"data\"][0][\"values\"] = df1.T.apply(lambda x: x.dropna().to_dict()).tolist()\n",
    "\n",
    "        del d1[\"data\"][-2][\"url\"]\n",
    "        d1[\"data\"][-2][\"values\"] = df2.T.apply(lambda x: x.dropna().to_dict()).tolist()\n",
    "    if not show_node_names:\n",
    "        d1[\"marks\"][0][\"encode\"][\"update\"][\"fontSize\"] = 0\n",
    "    if plot:\n",
    "        from parag.utils import display_plot\n",
    "\n",
    "        display_plot(d1.copy(), defaults=defaults)\n",
    "    if out is None:\n",
    "        return d1, df1\n",
    "    elif out == \"cfg\":\n",
    "        return d1\n",
    "    else:  # out=='df':\n",
    "        return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## demo data\n",
    "from parag.core import get_net_data\n",
    "import scanpy as sc\n",
    "nodes,edges=get_net_data(sc.datasets.pbmc68k_reduced())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_base,nodes_=to_net(\n",
    "    nodes=nodes,\n",
    "    edges=edges,\n",
    "    col_node_id='cell id',\n",
    "    col_source='cell id1',\n",
    "    col_target='cell id2',\n",
    "    col_subset_id='bulk_labels',\n",
    "    defaults=dict(\n",
    "        radius=165,\n",
    "        textSize=8,\n",
    "        textOffset=7,\n",
    "    ),\n",
    "    # show_node_names=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parag",
   "language": "python",
   "name": "parag"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
